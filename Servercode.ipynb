{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries\n",
        "!pip install scikit-learn flwr qiskit numpy pandas matplotlib joblib pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecyrE394GZz4",
        "outputId": "4e6cd7f1-5630-4e17-e812-ec80d98c344f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting flwr\n",
            "  Downloading flwr-1.13.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting qiskit\n",
            "  Downloading qiskit-1.3.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.39.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting cryptography<43.0.0,>=42.0.4 (from flwr)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting grpcio!=1.64.2,<2.0.0,<=1.64.3,>=1.60.0 (from flwr)\n",
            "  Downloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (4.25.5)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from flwr) (2.2.1)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.13.1)\n",
            "Collecting dill>=0.3 (from qiskit)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit) (4.12.2)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit)\n",
            "  Downloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Collecting pennylane-lightning>=0.39 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.4->flwr) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n",
            "  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Downloading flwr-1.13.1-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.2/512.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit-1.3.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane-0.39.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, tomli-w, symengine, rustworkx, pycryptodome, pbr, pathspec, iterators, grpcio, dill, autoray, stevedore, cryptography, typer, qiskit, flwr, pennylane-lightning, pennylane\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.68.1\n",
            "    Uninstalling grpcio-1.68.1:\n",
            "      Successfully uninstalled grpcio-1.68.1\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.1\n",
            "    Uninstalling typer-0.15.1:\n",
            "      Successfully uninstalled typer-0.15.1\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.0 cryptography-42.0.8 dill-0.3.9 flwr-1.13.1 grpcio-1.64.3 iterators-0.0.2 pathspec-0.12.1 pbr-6.1.0 pennylane-0.39.0 pennylane-lightning-0.39.0 pycryptodome-3.21.0 qiskit-1.3.1 rustworkx-0.15.1 stevedore-5.4.0 symengine-0.13.0 tomli-w-1.1.0 typer-0.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2FWbUKbFWWD",
        "outputId": "a04f4b1b-def4-47a1-f82c-2eb624e223d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1: Starting...\n",
            "Client 1: No Global Model Found. Training Fresh Model...\n",
            "Loading training dataset...\n",
            "Loading training dataset...\n",
            "Preprocessed training data saved!\n",
            "Preprocessed training data saved!\n",
            "Loading testing dataset...\n",
            "Loading preprocessed training data...\n",
            "Preprocessed testing data saved!\n",
            "Client 1: Epoch 1 - Training...\n",
            "Accuracy: 0.9998, Precision: 0.9987, Recall: 0.8346, F1 Score: 0.9093\n",
            "Client 1: Epoch 2 - Training...\n",
            "Accuracy: 0.9999, Precision: 1.0000, Recall: 0.9130, F1 Score: 0.9545\n",
            "Client 1: Epoch 3 - Training...\n",
            "Accuracy: 0.9999, Precision: 1.0000, Recall: 0.9554, F1 Score: 0.9772\n",
            "Client 1: Epoch 4 - Training...\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 0.9719, F1 Score: 0.9857\n",
            "Client 1: Epoch 5 - Training...\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 0.9835, F1 Score: 0.9917\n",
            "Client {CLIENT_ID}: Testing Local Model...\n",
            "Client 1: Final Testing Accuracy: 1.0000\n",
            "Client 1: Local Model Saved Successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import os\n",
        "import pennylane as qml\n",
        "import random\n",
        "import base64\n",
        "from cryptography.fernet import Fernet\n",
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------- Configuration --------------------\n",
        "ROLE = \"client\"  # Change to \"server\" when running on the server\n",
        "CLIENT_ID = 1  # Unique ID for the client\n",
        "DATASET_PATH = \"client_1_dataset.csv\"\n",
        "TEST_DATASET_PATH = \"client_3_dataset.csv\"  # Path to your testing datasetjbvubdiubvdbvsyubvsuyvvbxsuyvuivuifvtuivctuvu\n",
        "SERVER_MODEL_PATH = \"server_global_model.joblib\"\n",
        "LOCAL_MODEL_PATH = f\"client_{CLIENT_ID}_model_encrypted.joblib\"\n",
        "ENCRYPTION_KEY_PATH = \"encryption_key.key\"\n",
        "DECRYPTED_MODEL_PATH = f\"client_{CLIENT_ID}_model_decrypted.joblib\"\n",
        "NOISE_LEVEL = 0.001  # Quantum noise level\n",
        "NUM_QUBITS = 4\n",
        "\n",
        "# Quantum Device\n",
        "dev = qml.device(\"default.mixed\", wires=NUM_QUBITS)\n",
        "\n",
        "# -------------------- Quantum Differential Privacy --------------------\n",
        "@qml.qnode(dev)\n",
        "def quantum_noise(weights):\n",
        "    \"\"\"Simulate quantum noise.\"\"\"\n",
        "    for i in range(NUM_QUBITS):\n",
        "        qml.RX(weights[i], wires=i)\n",
        "        qml.RY(weights[i], wires=i)\n",
        "    qml.DepolarizingChannel(NOISE_LEVEL, wires=0)\n",
        "    qml.PhaseDamping(NOISE_LEVEL, wires=1)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(NUM_QUBITS)]\n",
        "\n",
        "def apply_qdp(gradients):\n",
        "    \"\"\"Apply Quantum Differential Privacy.\"\"\"\n",
        "    noisy_gradients = []\n",
        "    for grad in gradients:\n",
        "        weights = np.random.rand(NUM_QUBITS)\n",
        "        noise = sum(quantum_noise(weights)) / NUM_QUBITS\n",
        "        noisy_gradients.append(grad + noise)\n",
        "    return np.array(noisy_gradients)\n",
        "\n",
        "# -------------------- Encryption Utilities --------------------\n",
        "def load_or_create_key():\n",
        "    if os.path.exists(ENCRYPTION_KEY_PATH):\n",
        "        with open(ENCRYPTION_KEY_PATH, \"rb\") as file:\n",
        "            return file.read()\n",
        "    else:\n",
        "        key = Fernet.generate_key()\n",
        "        with open(ENCRYPTION_KEY_PATH, \"wb\") as file:\n",
        "            file.write(key)\n",
        "        return key\n",
        "\n",
        "def encrypt_model(model_path, encrypted_path, key):\n",
        "    \"\"\"Encrypt the model file.\"\"\"\n",
        "    fernet = Fernet(key)\n",
        "    with open(model_path, \"rb\") as file:\n",
        "        encrypted_data = fernet.encrypt(file.read())\n",
        "    with open(encrypted_path, \"wb\") as file:\n",
        "        file.write(encrypted_data)\n",
        "\n",
        "def decrypt_model(encrypted_path, decrypted_path, key):\n",
        "    \"\"\"Decrypt the model file.\"\"\"\n",
        "    fernet = Fernet(key)\n",
        "    with open(encrypted_path, \"rb\") as file:\n",
        "        decrypted_data = fernet.decrypt(file.read())\n",
        "    with open(decrypted_path, \"wb\") as file:\n",
        "        file.write(decrypted_data)\n",
        "\n",
        "# -------------------- Load and Preprocess Data --------------------\n",
        "def load_and_preprocess_data(train=True):\n",
        "    \"\"\"\n",
        "    Load and preprocess data.\n",
        "    If train=True, load the training dataset. Else, load the testing dataset.\n",
        "    \"\"\"\n",
        "    processed_file = \"preprocessed_data_train.joblib\" if train else \"preprocessed_data_test.joblib\"\n",
        "    dataset_path = DATASET_PATH if train else TEST_DATASET_PATH\n",
        "\n",
        "    if os.path.exists(processed_file):\n",
        "        print(f\"Loading preprocessed {'training' if train else 'testing'} data...\")\n",
        "        data = joblib.load(processed_file)\n",
        "        return data[\"X\"], data[\"y\"]\n",
        "\n",
        "    # Load the dataset\n",
        "    print(f\"Loading {'training' if train else 'testing'} dataset...\")\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Drop forbidden and irrelevant columns\n",
        "    df = df.drop(columns=['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'], errors='ignore')\n",
        "    df = df.drop(columns=['nameOrig', 'nameDest'], errors='ignore')\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
        "\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "    # Binary target column\n",
        "    df_imputed['isFraud'] = df_imputed['isFraud'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "    # Features and target\n",
        "    X = df_imputed.drop(columns=['isFraud'])\n",
        "    y = df_imputed['isFraud']\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    joblib.dump({\"X\": X_scaled, \"y\": y}, processed_file)\n",
        "    print(f\"Preprocessed {'training' if train else 'testing'} data saved!\")\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# -------------------- Client Node --------------------\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "def client_node():\n",
        "    print(f\"Client {CLIENT_ID}: Starting...\")\n",
        "    global_model_exists = os.path.exists(SERVER_MODEL_PATH)\n",
        "\n",
        "    if global_model_exists:\n",
        "        print(f\"Client {CLIENT_ID}: Loading Global Model from Server...\")\n",
        "        global_model = joblib.load(SERVER_MODEL_PATH)\n",
        "    else:\n",
        "        print(f\"Client {CLIENT_ID}: No Global Model Found. Training Fresh Model...\")\n",
        "        global_model = None\n",
        "\n",
        "    # Load and preprocess training data\n",
        "    print(\"Loading training dataset...\")\n",
        "    X_train, y_train = load_and_preprocess_data()\n",
        "    joblib.dump({\"X\": X_train, \"y\": y_train}, \"train_data_preprocessed.joblib\")\n",
        "    print(\"Preprocessed training data saved!\")\n",
        "\n",
        "    # Load and preprocess testing data\n",
        "    print(\"Loading testing dataset...\")\n",
        "    TEST_DATASET_PATH = \"test_dataset.csv\"  # Path to the testing dataset\n",
        "    X_test, y_test = load_and_preprocess_data()\n",
        "    joblib.dump({\"X\": X_test, \"y\": y_test}, \"test_data_preprocessed.joblib\")\n",
        "    print(\"Preprocessed testing data saved!\")\n",
        "\n",
        "    # Initialize or load the model\n",
        "    model = RandomForestClassifier(n_estimators=10, warm_start=True, random_state=42)\n",
        "\n",
        "    # Training Loop with Epochs\n",
        "    num_epochs = 5\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"Client {CLIENT_ID}: Epoch {epoch} - Training...\")\n",
        "\n",
        "        # Incrementally train the model\n",
        "        model.set_params(n_estimators=epoch * 10)  # Add 10 trees per epoch\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the training dataset\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        accuracy = accuracy_score(y_train, y_pred_train)\n",
        "        precision = precision_score(y_train, y_pred_train)\n",
        "        recall = recall_score(y_train, y_pred_train)\n",
        "        f1 = f1_score(y_train, y_pred_train)\n",
        "\n",
        "        # Print performance metrics\n",
        "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Final Test on the Testing Dataset\n",
        "    print(\"Client {CLIENT_ID}: Testing Local Model...\")\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    print(f\"Client {CLIENT_ID}: Final Testing Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(model, LOCAL_MODEL_PATH)\n",
        "    print(f\"Client {CLIENT_ID}: Local Model Saved Successfully!\")\n",
        "\n",
        "# -------------------- Server Node --------------------\n",
        "def server_node():\n",
        "    key = load_or_create_key()\n",
        "    print(\"Server: Starting Aggregation...\")\n",
        "    models = []\n",
        "    for client_id in range(1, 4):  # Assume 3 clients\n",
        "        encrypted_model_path = f\"client_{client_id}_model_encrypted.joblib\"\n",
        "        if os.path.exists(encrypted_model_path):\n",
        "            decrypt_model(encrypted_model_path, DECRYPTED_MODEL_PATH, key)\n",
        "            model = joblib.load(DECRYPTED_MODEL_PATH)\n",
        "            models.append(model)\n",
        "            print(f\"Server: Model from Client {client_id} Loaded.\")\n",
        "    if not models:\n",
        "        print(\"Server: No models found!\")\n",
        "        return\n",
        "\n",
        "    # Aggregate models\n",
        "    avg_importances = np.mean([model.feature_importances_ for model in models], axis=0)\n",
        "    global_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    global_model.fit(np.zeros((10, len(avg_importances))), np.zeros(10))  # Dummy data\n",
        "    global_model.feature_importances_ = avg_importances\n",
        "\n",
        "    # Save global model\n",
        "    joblib.dump(global_model, SERVER_MODEL_PATH)\n",
        "    encrypt_model(SERVER_MODEL_PATH, SERVER_MODEL_PATH, key)\n",
        "    print(\"Server: Global Model Aggregated and Saved.\")\n",
        "\n",
        "# -------------------- Execution --------------------\n",
        "if ROLE == \"client\":\n",
        "    client_node()\n",
        "elif ROLE == \"server\":\n",
        "    server_node()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import os\n",
        "import pennylane as qml\n",
        "import random\n",
        "from cryptography.fernet import Fernet\n",
        "\n",
        "# -------------------- Configuration --------------------\n",
        "ROLE = \"server\"  # Change to \"server\" when running on the server\n",
        "CLIENT_ID = 1  # Unique ID for the client\n",
        "DATASET_PATH = \"client_1_dataset.csv\"\n",
        "TEST_DATASET_PATH = \"test_dataset.csv\"  # Path to your testing dataset\n",
        "SERVER_MODEL_PATH = \"server_global_model.joblib\"\n",
        "LOCAL_MODEL_PATH = f\"client_{CLIENT_ID}_model_encrypted.joblib\"\n",
        "ENCRYPTION_KEY_PATH = \"encryption_key.key\"\n",
        "DECRYPTED_MODEL_PATH = f\"client_{CLIENT_ID}_model_decrypted.joblib\"\n",
        "NOISE_LEVEL = 0.001  # Quantum noise level\n",
        "NUM_QUBITS = 4\n",
        "\n",
        "# Quantum Device\n",
        "dev = qml.device(\"default.mixed\", wires=NUM_QUBITS)\n",
        "\n",
        "# -------------------- Encryption Utilities --------------------\n",
        "def load_or_create_key():\n",
        "    if os.path.exists(ENCRYPTION_KEY_PATH):\n",
        "        with open(ENCRYPTION_KEY_PATH, \"rb\") as file:\n",
        "            return file.read()\n",
        "    else:\n",
        "        key = Fernet.generate_key()\n",
        "        with open(ENCRYPTION_KEY_PATH, \"wb\") as file:\n",
        "            file.write(key)\n",
        "        return key\n",
        "\n",
        "def decrypt_model(encrypted_path, decrypted_path, key):\n",
        "    \"\"\"Decrypt the model file.\"\"\"\n",
        "    fernet = Fernet(key)\n",
        "    with open(encrypted_path, \"rb\") as file:\n",
        "        decrypted_data = fernet.decrypt(file.read())\n",
        "    with open(decrypted_path, \"wb\") as file:\n",
        "        file.write(decrypted_data)\n",
        "\n",
        "# -------------------- Load and Preprocess Data --------------------\n",
        "def load_and_preprocess_data(train=True):\n",
        "    processed_file = \"preprocessed_data_train.joblib\" if train else \"preprocessed_data_test.joblib\"\n",
        "    dataset_path = DATASET_PATH if train else TEST_DATASET_PATH\n",
        "\n",
        "    if os.path.exists(processed_file):\n",
        "        print(f\"Loading preprocessed {'training' if train else 'testing'} data...\")\n",
        "        data = joblib.load(processed_file)\n",
        "        return data[\"X\"], data[\"y\"]\n",
        "\n",
        "    # Load the dataset\n",
        "    print(f\"Loading {'training' if train else 'testing'} dataset...\")\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Drop forbidden and irrelevant columns\n",
        "    df = df.drop(columns=['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'], errors='ignore')\n",
        "    df = df.drop(columns=['nameOrig', 'nameDest'], errors='ignore')\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
        "\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "    # Binary target column\n",
        "    df_imputed['isFraud'] = df_imputed['isFraud'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "    # Features and target\n",
        "    X = df_imputed.drop(columns=['isFraud'])\n",
        "    y = df_imputed['isFraud']\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    joblib.dump({\"X\": X_scaled, \"y\": y}, processed_file)\n",
        "    print(f\"Preprocessed {'training' if train else 'testing'} data saved!\")\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# -------------------- Server Node --------------------\n",
        "def server_node():\n",
        "    key = load_or_create_key()\n",
        "    print(\"Server: Starting Aggregation...\")\n",
        "    models = []\n",
        "    for client_id in range(1, 4):  # Assume 3 clients\n",
        "        encrypted_model_path = f\"client_{client_id}_model_encrypted.joblib\"\n",
        "        if os.path.exists(encrypted_model_path):\n",
        "            decrypt_model(encrypted_model_path, DECRYPTED_MODEL_PATH, key)\n",
        "            model = joblib.load(DECRYPTED_MODEL_PATH)\n",
        "            models.append(model)\n",
        "            print(f\"Server: Model from Client {client_id} Loaded.\")\n",
        "    if not models:\n",
        "        print(\"Server: No models found!\")\n",
        "        return\n",
        "\n",
        "    # Aggregate models\n",
        "    avg_importances = np.mean([model.feature_importances_ for model in models], axis=0)\n",
        "    global_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    global_model.fit(np.zeros((10, len(avg_importances))), np.zeros(10))  # Dummy data\n",
        "    global_model.feature_importances_ = avg_importances\n",
        "\n",
        "    # Save global model\n",
        "    joblib.dump(global_model, SERVER_MODEL_PATH)\n",
        "    print(\"Server: Global Model Aggregated and Saved.\")\n",
        "\n",
        "    # Test the global model on the testing dataset\n",
        "    print(\"Testing the Global Model...\")\n",
        "    X_test, y_test = load_and_preprocess_data(train=False)  # Load testing data\n",
        "    y_pred_test = global_model.predict(X_test)\n",
        "\n",
        "    # Calculate confusion matrix and related metrics\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "    accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    precision = precision_score(y_test, y_pred_test)\n",
        "    recall = recall_score(y_test, y_pred_test)\n",
        "    f1 = f1_score(y_test, y_pred_test)\n",
        "\n",
        "    # Print confusion matrix and metrics\n",
        "    print(f\"Confusion Matrix:\\n TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# -------------------- Execution --------------------\n",
        "if ROLE == \"server\":\n",
        "    server_node()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "knwF4TJXf9cL",
        "outputId": "defca1a1-c9a9-4f0c-a458-01b77fe85f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server: Starting Aggregation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidToken",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidToken\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b0d1dc10a970>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# -------------------- Execution --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mROLE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"server\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mserver_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-b0d1dc10a970>\u001b[0m in \u001b[0;36mserver_node\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mencrypted_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"client_{client_id}_model_encrypted.joblib\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mdecrypt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDECRYPTED_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDECRYPTED_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b0d1dc10a970>\u001b[0m in \u001b[0;36mdecrypt_model\u001b[0;34m(encrypted_path, decrypted_path, key)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mfernet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFernet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mdecrypted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfernet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecrypted_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecrypted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cryptography/fernet.py\u001b[0m in \u001b[0;36mdecrypt\u001b[0;34m(self, token, ttl)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFernet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unverified_token_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mttl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtime_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cryptography/fernet.py\u001b[0m in \u001b[0;36m_get_unverified_token_data\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0x80\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidToken\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import os\n",
        "import pennylane as qml\n",
        "import random\n",
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------- Configuration --------------------\n",
        "ROLE = \"client\"  # Change to \"server\" when running on the server\n",
        "CLIENT_ID = 1  # Unique ID for the client\n",
        "DATASET_PATH = \"client_1_dataset.csv\"\n",
        "TEST_DATASET_PATH = \"client_3_dataset.csv\"  # Path to your testing dataset\n",
        "SERVER_MODEL_PATH = \"server_global_model.joblib\"\n",
        "LOCAL_MODEL_PATH = f\"client_{CLIENT_ID}_model.joblib\"  # No encryption for local model\n",
        "NOISE_LEVEL = 0.001  # Quantum noise level\n",
        "NUM_QUBITS = 4\n",
        "\n",
        "# Quantum Device\n",
        "dev = qml.device(\"default.mixed\", wires=NUM_QUBITS)\n",
        "\n",
        "# -------------------- Quantum Differential Privacy --------------------\n",
        "@qml.qnode(dev)\n",
        "def quantum_noise(weights):\n",
        "    \"\"\"Simulate quantum noise.\"\"\"\n",
        "    for i in range(NUM_QUBITS):\n",
        "        qml.RX(weights[i], wires=i)\n",
        "        qml.RY(weights[i], wires=i)\n",
        "    qml.DepolarizingChannel(NOISE_LEVEL, wires=0)\n",
        "    qml.PhaseDamping(NOISE_LEVEL, wires=1)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(NUM_QUBITS)]\n",
        "\n",
        "def apply_qdp(gradients):\n",
        "    \"\"\"Apply Quantum Differential Privacy.\"\"\"\n",
        "    noisy_gradients = []\n",
        "    for grad in gradients:\n",
        "        weights = np.random.rand(NUM_QUBITS)\n",
        "        noise = sum(quantum_noise(weights)) / NUM_QUBITS\n",
        "        noisy_gradients.append(grad + noise)\n",
        "    return np.array(noisy_gradients)\n",
        "\n",
        "# -------------------- Load and Preprocess Data --------------------\n",
        "def load_and_preprocess_data(train=True):\n",
        "    \"\"\"\n",
        "    Load and preprocess data.\n",
        "    If train=True, load the training dataset. Else, load the testing dataset.\n",
        "    \"\"\"\n",
        "    processed_file = \"preprocessed_data_train.joblib\" if train else \"preprocessed_data_test.joblib\"\n",
        "    dataset_path = DATASET_PATH if train else TEST_DATASET_PATH\n",
        "\n",
        "    if os.path.exists(processed_file):\n",
        "        print(f\"Loading preprocessed {'training' if train else 'testing'} data...\")\n",
        "        data = joblib.load(processed_file)\n",
        "        return data[\"X\"], data[\"y\"]\n",
        "\n",
        "    # Load the dataset\n",
        "    print(f\"Loading {'training' if train else 'testing'} dataset...\")\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Drop forbidden and irrelevant columns\n",
        "    df = df.drop(columns=['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'], errors='ignore')\n",
        "    df = df.drop(columns=['nameOrig', 'nameDest'], errors='ignore')\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
        "\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "    # Binary target column\n",
        "    df_imputed['isFraud'] = df_imputed['isFraud'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "    # Features and target\n",
        "    X = df_imputed.drop(columns=['isFraud'])\n",
        "    y = df_imputed['isFraud']\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    joblib.dump({\"X\": X_scaled, \"y\": y}, processed_file)\n",
        "    print(f\"Preprocessed {'training' if train else 'testing'} data saved!\")\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# -------------------- Client Node --------------------\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "def client_node():\n",
        "    print(f\"Client {CLIENT_ID}: Starting...\")\n",
        "    global_model_exists = os.path.exists(SERVER_MODEL_PATH)\n",
        "\n",
        "    if global_model_exists:\n",
        "        print(f\"Client {CLIENT_ID}: Loading Global Model from Server...\")\n",
        "        global_model = joblib.load(SERVER_MODEL_PATH)\n",
        "    else:\n",
        "        print(f\"Client {CLIENT_ID}: No Global Model Found. Training Fresh Model...\")\n",
        "        global_model = None\n",
        "\n",
        "    # Load and preprocess training data\n",
        "    print(\"Loading training dataset...\")\n",
        "    X_train, y_train = load_and_preprocess_data()\n",
        "    joblib.dump({\"X\": X_train, \"y\": y_train}, \"train_data_preprocessed.joblib\")\n",
        "    print(\"Preprocessed training data saved!\")\n",
        "\n",
        "    # Load and preprocess testing data\n",
        "    print(\"Loading testing dataset...\")\n",
        "    TEST_DATASET_PATH = \"test_dataset.csv\"  # Path to the testing dataset\n",
        "    X_test, y_test = load_and_preprocess_data()\n",
        "    joblib.dump({\"X\": X_test, \"y\": y_test}, \"test_data_preprocessed.joblib\")\n",
        "    print(\"Preprocessed testing data saved!\")\n",
        "\n",
        "    # Initialize or load the model\n",
        "    model = RandomForestClassifier(n_estimators=10, warm_start=True, random_state=42)\n",
        "\n",
        "    # Training Loop with Epochs\n",
        "    num_epochs = 5\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"Client {CLIENT_ID}: Epoch {epoch} - Training...\")\n",
        "\n",
        "        # Incrementally train the model\n",
        "        model.set_params(n_estimators=epoch * 10)  # Add 10 trees per epoch\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the training dataset\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        accuracy = accuracy_score(y_train, y_pred_train)\n",
        "        precision = precision_score(y_train, y_pred_train)\n",
        "        recall = recall_score(y_train, y_pred_train)\n",
        "        f1 = f1_score(y_train, y_pred_train)\n",
        "\n",
        "        # Print performance metrics\n",
        "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Final Test on the Testing Dataset\n",
        "    print(\"Client {CLIENT_ID}: Testing Local Model...\")\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    print(f\"Client {CLIENT_ID}: Final Testing Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(model, LOCAL_MODEL_PATH)\n",
        "    print(f\"Client {CLIENT_ID}: Local Model Saved Successfully!\")\n",
        "\n",
        "# -------------------- Server Node --------------------\n",
        "def server_node():\n",
        "    print(\"Server: Starting Aggregation...\")\n",
        "    models = []\n",
        "    for client_id in range(1, 4):  # Assume 3 clients\n",
        "        model_path = f\"client_{client_id}_model.joblib\"\n",
        "        if os.path.exists(model_path):\n",
        "            model = joblib.load(model_path)\n",
        "            models.append(model)\n",
        "            print(f\"Server: Model from Client {client_id} Loaded.\")\n",
        "\n",
        "    if not models:\n",
        "        print(\"Server: No models found!\")\n",
        "        return\n",
        "\n",
        "    # Aggregate models\n",
        "    avg_importances = np.mean([model.feature_importances_ for model in models], axis=0)\n",
        "    global_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    global_model.fit(np.zeros((10, len(avg_importances))), np.zeros(10))  # Dummy data\n",
        "    global_model.feature_importances_ = avg_importances\n",
        "\n",
        "    # Save global model\n",
        "    joblib.dump(global_model, SERVER_MODEL_PATH)\n",
        "    print(\"Server: Global Model Aggregated and Saved.\")\n",
        "\n",
        "# -------------------- Execution --------------------\n",
        "if ROLE == \"client\":\n",
        "    client_node()\n",
        "elif ROLE == \"server\":\n",
        "    server_node()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "0FUHwbXHlXIv",
        "outputId": "ed1c5e23-14bb-4133-aad2-79bc9a3e1376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1: Starting...\n",
            "Client 1: No Global Model Found. Training Fresh Model...\n",
            "Loading training dataset...\n",
            "Loading training dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'client_1_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-077f823ca14b>\u001b[0m in \u001b[0;36m<cell line: 179>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m# -------------------- Execution --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mROLE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"client\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mclient_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mROLE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"server\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mserver_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-077f823ca14b>\u001b[0m in \u001b[0;36mclient_node\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Load and preprocess training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading training dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_data_preprocessed.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessed training data saved!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-077f823ca14b>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {'training' if train else 'testing'} dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Drop forbidden and irrelevant columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_1_dataset.csv'"
          ]
        }
      ]
    }
  ]
}